{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uX4bpEJuX7W"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VSO16K9iuX7W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_UAz54JuyUv"
      },
      "source": [
        "### Create directory to save generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OQW2IhBpuX7X"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"gan_images\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YItGLRHuX7X"
      },
      "source": [
        "## Define the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "enfKsPiJuX7X"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 28*28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), 1, 28, 28)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqnMkxkuX7X"
      },
      "source": [
        "# Define the Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wARfTJfguX7Y"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn2TI6ZAuX7Y"
      },
      "source": [
        "## Initialize Models and Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKJIJ7p_uX7Y"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d-EJ4AxMuX7Y"
      },
      "outputs": [],
      "source": [
        "latent_dim = 150\n",
        "lr = 0.0002\n",
        "batch_size = 64\n",
        "n_epochs = 70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sStnDUA6uX7Y"
      },
      "source": [
        "### Initialize models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cWevUxfNuX7Y"
      },
      "outputs": [],
      "source": [
        "generator = Generator(latent_dim)\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSGCjrh2uX7Y"
      },
      "source": [
        "### Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v-eWw7rauX7Y"
      },
      "outputs": [],
      "source": [
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nwUJaG4uX7Y"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0kBsWL2TuX7Z"
      },
      "outputs": [],
      "source": [
        "adversarial_loss = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bpiOLgFuX7Z"
      },
      "source": [
        "### Check for CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXyTTmZHuX7Z",
        "outputId": "b9783009-9089-401f-e1b7-03b415ba1036"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BCELoss()"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "adversarial_loss.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-4zxjzPuX7Z"
      },
      "source": [
        "## Load the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia6-uwpRuX7Z",
        "outputId": "58d4a180-942a-44c8-95a4-2a7d82e7efdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 11616786.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 346612.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3203990.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4709648.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataloader = DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"./data/mnist\", train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "    ),\n",
        "    batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERiDnEnYuX7Z"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46DW1NvruX7Z",
        "outputId": "113326a4-8ccb-4863-fb2e-a0d7bedbd195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0/70] [Batch 0/938] [D loss: 0.6889] [G loss: 0.6627]\n",
            "[Epoch 0/70] [Batch 100/938] [D loss: 0.3352] [G loss: 1.0011]\n",
            "[Epoch 0/70] [Batch 200/938] [D loss: 0.5301] [G loss: 0.7996]\n",
            "[Epoch 0/70] [Batch 300/938] [D loss: 0.5558] [G loss: 0.9461]\n",
            "[Epoch 0/70] [Batch 400/938] [D loss: 0.4089] [G loss: 0.9666]\n",
            "[Epoch 0/70] [Batch 500/938] [D loss: 0.4014] [G loss: 0.9066]\n",
            "[Epoch 0/70] [Batch 600/938] [D loss: 0.4180] [G loss: 0.9108]\n",
            "[Epoch 0/70] [Batch 700/938] [D loss: 0.7227] [G loss: 2.5288]\n",
            "[Epoch 0/70] [Batch 800/938] [D loss: 0.4996] [G loss: 0.8333]\n",
            "[Epoch 0/70] [Batch 900/938] [D loss: 0.4625] [G loss: 1.0352]\n",
            "[Epoch 1/70] [Batch 0/938] [D loss: 0.3608] [G loss: 1.1718]\n",
            "[Epoch 1/70] [Batch 100/938] [D loss: 0.4698] [G loss: 0.5990]\n",
            "[Epoch 1/70] [Batch 200/938] [D loss: 0.3886] [G loss: 1.2262]\n",
            "[Epoch 1/70] [Batch 300/938] [D loss: 0.4225] [G loss: 0.7467]\n",
            "[Epoch 1/70] [Batch 400/938] [D loss: 0.4213] [G loss: 1.6008]\n",
            "[Epoch 1/70] [Batch 500/938] [D loss: 0.3810] [G loss: 2.0667]\n",
            "[Epoch 1/70] [Batch 600/938] [D loss: 0.4539] [G loss: 1.1767]\n",
            "[Epoch 1/70] [Batch 700/938] [D loss: 0.3472] [G loss: 0.8484]\n",
            "[Epoch 1/70] [Batch 800/938] [D loss: 0.3080] [G loss: 1.3358]\n",
            "[Epoch 1/70] [Batch 900/938] [D loss: 0.3111] [G loss: 1.1207]\n",
            "[Epoch 2/70] [Batch 0/938] [D loss: 0.2785] [G loss: 1.2737]\n",
            "[Epoch 2/70] [Batch 100/938] [D loss: 0.4157] [G loss: 0.7245]\n",
            "[Epoch 2/70] [Batch 200/938] [D loss: 0.2964] [G loss: 1.6529]\n",
            "[Epoch 2/70] [Batch 300/938] [D loss: 0.6712] [G loss: 0.3618]\n",
            "[Epoch 2/70] [Batch 400/938] [D loss: 0.5188] [G loss: 2.4401]\n",
            "[Epoch 2/70] [Batch 500/938] [D loss: 0.2920] [G loss: 1.5593]\n",
            "[Epoch 2/70] [Batch 600/938] [D loss: 0.3461] [G loss: 0.9599]\n",
            "[Epoch 2/70] [Batch 700/938] [D loss: 0.2367] [G loss: 1.5800]\n",
            "[Epoch 2/70] [Batch 800/938] [D loss: 0.4430] [G loss: 0.6631]\n",
            "[Epoch 2/70] [Batch 900/938] [D loss: 0.3681] [G loss: 0.9317]\n",
            "[Epoch 3/70] [Batch 0/938] [D loss: 0.4642] [G loss: 0.8997]\n",
            "[Epoch 3/70] [Batch 100/938] [D loss: 0.3871] [G loss: 0.7384]\n",
            "[Epoch 3/70] [Batch 200/938] [D loss: 0.2457] [G loss: 1.6410]\n",
            "[Epoch 3/70] [Batch 300/938] [D loss: 0.3619] [G loss: 0.9125]\n",
            "[Epoch 3/70] [Batch 400/938] [D loss: 0.5494] [G loss: 0.4624]\n",
            "[Epoch 3/70] [Batch 500/938] [D loss: 0.4129] [G loss: 0.8380]\n",
            "[Epoch 3/70] [Batch 600/938] [D loss: 0.3903] [G loss: 2.4551]\n",
            "[Epoch 3/70] [Batch 700/938] [D loss: 0.3382] [G loss: 0.9051]\n",
            "[Epoch 3/70] [Batch 800/938] [D loss: 0.4509] [G loss: 0.6420]\n",
            "[Epoch 3/70] [Batch 900/938] [D loss: 0.2401] [G loss: 2.2759]\n",
            "[Epoch 4/70] [Batch 0/938] [D loss: 0.2986] [G loss: 0.9500]\n",
            "[Epoch 4/70] [Batch 100/938] [D loss: 0.3233] [G loss: 3.5125]\n",
            "[Epoch 4/70] [Batch 200/938] [D loss: 0.1264] [G loss: 2.6384]\n",
            "[Epoch 4/70] [Batch 300/938] [D loss: 0.2590] [G loss: 2.1854]\n",
            "[Epoch 4/70] [Batch 400/938] [D loss: 0.2919] [G loss: 1.0895]\n",
            "[Epoch 4/70] [Batch 500/938] [D loss: 0.3038] [G loss: 0.9008]\n",
            "[Epoch 4/70] [Batch 600/938] [D loss: 0.2006] [G loss: 2.7876]\n",
            "[Epoch 4/70] [Batch 700/938] [D loss: 0.3127] [G loss: 3.5545]\n",
            "[Epoch 4/70] [Batch 800/938] [D loss: 0.3195] [G loss: 1.1403]\n",
            "[Epoch 4/70] [Batch 900/938] [D loss: 0.3571] [G loss: 1.0971]\n",
            "[Epoch 5/70] [Batch 0/938] [D loss: 0.3870] [G loss: 0.7144]\n",
            "[Epoch 5/70] [Batch 100/938] [D loss: 0.2043] [G loss: 2.2501]\n",
            "[Epoch 5/70] [Batch 200/938] [D loss: 0.5068] [G loss: 0.5133]\n",
            "[Epoch 5/70] [Batch 300/938] [D loss: 0.1585] [G loss: 2.4073]\n",
            "[Epoch 5/70] [Batch 400/938] [D loss: 0.2401] [G loss: 2.3283]\n",
            "[Epoch 5/70] [Batch 500/938] [D loss: 0.1985] [G loss: 2.1060]\n",
            "[Epoch 5/70] [Batch 600/938] [D loss: 0.2465] [G loss: 1.7603]\n",
            "[Epoch 5/70] [Batch 700/938] [D loss: 0.2081] [G loss: 1.4011]\n",
            "[Epoch 5/70] [Batch 800/938] [D loss: 1.7984] [G loss: 8.0850]\n",
            "[Epoch 5/70] [Batch 900/938] [D loss: 0.2394] [G loss: 1.9628]\n",
            "[Epoch 6/70] [Batch 0/938] [D loss: 0.5300] [G loss: 4.0975]\n",
            "[Epoch 6/70] [Batch 100/938] [D loss: 0.7607] [G loss: 6.1055]\n",
            "[Epoch 6/70] [Batch 200/938] [D loss: 0.3719] [G loss: 2.2023]\n",
            "[Epoch 6/70] [Batch 300/938] [D loss: 0.5512] [G loss: 0.4768]\n",
            "[Epoch 6/70] [Batch 400/938] [D loss: 0.2685] [G loss: 2.2082]\n",
            "[Epoch 6/70] [Batch 500/938] [D loss: 0.9166] [G loss: 6.9648]\n",
            "[Epoch 6/70] [Batch 600/938] [D loss: 0.1416] [G loss: 2.2286]\n",
            "[Epoch 6/70] [Batch 700/938] [D loss: 0.2110] [G loss: 1.3803]\n",
            "[Epoch 6/70] [Batch 800/938] [D loss: 0.1885] [G loss: 1.4733]\n",
            "[Epoch 6/70] [Batch 900/938] [D loss: 0.3225] [G loss: 1.9247]\n",
            "[Epoch 7/70] [Batch 0/938] [D loss: 0.2103] [G loss: 1.3446]\n",
            "[Epoch 7/70] [Batch 100/938] [D loss: 0.4501] [G loss: 4.2731]\n",
            "[Epoch 7/70] [Batch 200/938] [D loss: 0.4188] [G loss: 4.1856]\n",
            "[Epoch 7/70] [Batch 300/938] [D loss: 0.1951] [G loss: 2.4609]\n",
            "[Epoch 7/70] [Batch 400/938] [D loss: 0.2242] [G loss: 1.3783]\n",
            "[Epoch 7/70] [Batch 500/938] [D loss: 0.1123] [G loss: 2.1437]\n",
            "[Epoch 7/70] [Batch 600/938] [D loss: 0.2100] [G loss: 1.3287]\n",
            "[Epoch 7/70] [Batch 700/938] [D loss: 0.3161] [G loss: 1.3114]\n",
            "[Epoch 7/70] [Batch 800/938] [D loss: 0.7567] [G loss: 6.4732]\n",
            "[Epoch 7/70] [Batch 900/938] [D loss: 0.3364] [G loss: 4.8649]\n",
            "[Epoch 8/70] [Batch 0/938] [D loss: 0.2278] [G loss: 1.3890]\n",
            "[Epoch 8/70] [Batch 100/938] [D loss: 0.1815] [G loss: 3.2710]\n",
            "[Epoch 8/70] [Batch 200/938] [D loss: 0.6380] [G loss: 5.6481]\n",
            "[Epoch 8/70] [Batch 300/938] [D loss: 0.2233] [G loss: 2.7743]\n",
            "[Epoch 8/70] [Batch 400/938] [D loss: 0.1640] [G loss: 2.2061]\n",
            "[Epoch 8/70] [Batch 500/938] [D loss: 0.1152] [G loss: 3.2150]\n",
            "[Epoch 8/70] [Batch 600/938] [D loss: 0.1364] [G loss: 3.9642]\n",
            "[Epoch 8/70] [Batch 700/938] [D loss: 0.3447] [G loss: 2.8494]\n",
            "[Epoch 8/70] [Batch 800/938] [D loss: 0.2298] [G loss: 4.0995]\n",
            "[Epoch 8/70] [Batch 900/938] [D loss: 0.2475] [G loss: 4.1567]\n",
            "[Epoch 9/70] [Batch 0/938] [D loss: 0.1731] [G loss: 3.1470]\n",
            "[Epoch 9/70] [Batch 100/938] [D loss: 0.1010] [G loss: 2.7237]\n",
            "[Epoch 9/70] [Batch 200/938] [D loss: 0.1843] [G loss: 1.6109]\n",
            "[Epoch 9/70] [Batch 300/938] [D loss: 0.1554] [G loss: 1.7889]\n",
            "[Epoch 9/70] [Batch 400/938] [D loss: 0.1305] [G loss: 2.2246]\n",
            "[Epoch 9/70] [Batch 500/938] [D loss: 0.1747] [G loss: 4.7035]\n",
            "[Epoch 9/70] [Batch 600/938] [D loss: 0.1996] [G loss: 1.4832]\n",
            "[Epoch 9/70] [Batch 700/938] [D loss: 0.1445] [G loss: 2.8965]\n",
            "[Epoch 9/70] [Batch 800/938] [D loss: 0.5659] [G loss: 5.2509]\n",
            "[Epoch 9/70] [Batch 900/938] [D loss: 0.2927] [G loss: 0.9931]\n",
            "[Epoch 10/70] [Batch 0/938] [D loss: 0.3188] [G loss: 6.3494]\n",
            "[Epoch 10/70] [Batch 100/938] [D loss: 0.2794] [G loss: 1.7539]\n",
            "[Epoch 10/70] [Batch 200/938] [D loss: 0.1653] [G loss: 2.2921]\n",
            "[Epoch 10/70] [Batch 300/938] [D loss: 0.2060] [G loss: 2.3527]\n",
            "[Epoch 10/70] [Batch 400/938] [D loss: 0.2820] [G loss: 1.3617]\n",
            "[Epoch 10/70] [Batch 500/938] [D loss: 0.0817] [G loss: 3.8233]\n",
            "[Epoch 10/70] [Batch 600/938] [D loss: 0.1912] [G loss: 2.2407]\n",
            "[Epoch 10/70] [Batch 700/938] [D loss: 0.1806] [G loss: 1.3746]\n",
            "[Epoch 10/70] [Batch 800/938] [D loss: 0.2794] [G loss: 2.7752]\n",
            "[Epoch 10/70] [Batch 900/938] [D loss: 0.1025] [G loss: 3.0836]\n",
            "[Epoch 11/70] [Batch 0/938] [D loss: 0.1859] [G loss: 6.2942]\n",
            "[Epoch 11/70] [Batch 100/938] [D loss: 0.2243] [G loss: 1.3766]\n",
            "[Epoch 11/70] [Batch 200/938] [D loss: 0.1707] [G loss: 1.8438]\n",
            "[Epoch 11/70] [Batch 300/938] [D loss: 0.1981] [G loss: 3.1428]\n",
            "[Epoch 11/70] [Batch 400/938] [D loss: 0.1735] [G loss: 2.4732]\n",
            "[Epoch 11/70] [Batch 500/938] [D loss: 0.2625] [G loss: 1.2258]\n",
            "[Epoch 11/70] [Batch 600/938] [D loss: 0.6524] [G loss: 7.3215]\n",
            "[Epoch 11/70] [Batch 700/938] [D loss: 0.1812] [G loss: 1.7595]\n",
            "[Epoch 11/70] [Batch 800/938] [D loss: 0.1416] [G loss: 3.7332]\n",
            "[Epoch 11/70] [Batch 900/938] [D loss: 0.5626] [G loss: 7.1905]\n",
            "[Epoch 12/70] [Batch 0/938] [D loss: 0.5494] [G loss: 6.7155]\n",
            "[Epoch 12/70] [Batch 100/938] [D loss: 0.1168] [G loss: 2.3033]\n",
            "[Epoch 12/70] [Batch 200/938] [D loss: 0.1247] [G loss: 2.7553]\n",
            "[Epoch 12/70] [Batch 300/938] [D loss: 0.0821] [G loss: 3.3512]\n",
            "[Epoch 12/70] [Batch 400/938] [D loss: 0.2814] [G loss: 4.5093]\n",
            "[Epoch 12/70] [Batch 500/938] [D loss: 0.2500] [G loss: 3.1813]\n",
            "[Epoch 12/70] [Batch 600/938] [D loss: 0.0734] [G loss: 3.1847]\n",
            "[Epoch 12/70] [Batch 700/938] [D loss: 0.2492] [G loss: 1.1810]\n",
            "[Epoch 12/70] [Batch 800/938] [D loss: 0.1347] [G loss: 3.0901]\n",
            "[Epoch 12/70] [Batch 900/938] [D loss: 0.1064] [G loss: 3.4238]\n",
            "[Epoch 13/70] [Batch 0/938] [D loss: 0.0891] [G loss: 4.1553]\n",
            "[Epoch 13/70] [Batch 100/938] [D loss: 0.1866] [G loss: 1.6729]\n",
            "[Epoch 13/70] [Batch 200/938] [D loss: 0.2367] [G loss: 4.0855]\n",
            "[Epoch 13/70] [Batch 300/938] [D loss: 0.2613] [G loss: 1.9012]\n",
            "[Epoch 13/70] [Batch 400/938] [D loss: 0.1757] [G loss: 1.5074]\n",
            "[Epoch 13/70] [Batch 500/938] [D loss: 0.1543] [G loss: 3.6134]\n",
            "[Epoch 13/70] [Batch 600/938] [D loss: 0.1308] [G loss: 2.0194]\n",
            "[Epoch 13/70] [Batch 700/938] [D loss: 0.0582] [G loss: 2.7510]\n",
            "[Epoch 13/70] [Batch 800/938] [D loss: 0.2599] [G loss: 5.1713]\n",
            "[Epoch 13/70] [Batch 900/938] [D loss: 0.1848] [G loss: 2.3280]\n",
            "[Epoch 14/70] [Batch 0/938] [D loss: 0.6541] [G loss: 8.0437]\n",
            "[Epoch 14/70] [Batch 100/938] [D loss: 0.1335] [G loss: 2.1887]\n",
            "[Epoch 14/70] [Batch 200/938] [D loss: 0.1284] [G loss: 2.1043]\n",
            "[Epoch 14/70] [Batch 300/938] [D loss: 0.2049] [G loss: 2.3986]\n",
            "[Epoch 14/70] [Batch 400/938] [D loss: 0.1804] [G loss: 3.6460]\n",
            "[Epoch 14/70] [Batch 500/938] [D loss: 0.2562] [G loss: 5.7751]\n",
            "[Epoch 14/70] [Batch 600/938] [D loss: 0.2556] [G loss: 1.1679]\n",
            "[Epoch 14/70] [Batch 700/938] [D loss: 0.1295] [G loss: 3.4124]\n",
            "[Epoch 14/70] [Batch 800/938] [D loss: 0.4978] [G loss: 5.8674]\n",
            "[Epoch 14/70] [Batch 900/938] [D loss: 0.1118] [G loss: 2.1959]\n",
            "[Epoch 15/70] [Batch 0/938] [D loss: 0.2964] [G loss: 1.0488]\n",
            "[Epoch 15/70] [Batch 100/938] [D loss: 0.1757] [G loss: 1.8461]\n",
            "[Epoch 15/70] [Batch 200/938] [D loss: 0.2364] [G loss: 4.2188]\n",
            "[Epoch 15/70] [Batch 300/938] [D loss: 0.1193] [G loss: 2.5505]\n",
            "[Epoch 15/70] [Batch 400/938] [D loss: 0.1347] [G loss: 2.4858]\n",
            "[Epoch 15/70] [Batch 500/938] [D loss: 0.2556] [G loss: 4.9150]\n",
            "[Epoch 15/70] [Batch 600/938] [D loss: 0.1232] [G loss: 2.2245]\n",
            "[Epoch 15/70] [Batch 700/938] [D loss: 0.2123] [G loss: 2.8998]\n",
            "[Epoch 15/70] [Batch 800/938] [D loss: 0.1397] [G loss: 3.7579]\n",
            "[Epoch 15/70] [Batch 900/938] [D loss: 0.1262] [G loss: 2.4371]\n",
            "[Epoch 16/70] [Batch 0/938] [D loss: 0.1560] [G loss: 2.6412]\n",
            "[Epoch 16/70] [Batch 100/938] [D loss: 0.1735] [G loss: 3.5298]\n",
            "[Epoch 16/70] [Batch 200/938] [D loss: 0.1823] [G loss: 2.4629]\n",
            "[Epoch 16/70] [Batch 300/938] [D loss: 0.2736] [G loss: 1.6611]\n",
            "[Epoch 16/70] [Batch 400/938] [D loss: 0.2135] [G loss: 1.4264]\n",
            "[Epoch 16/70] [Batch 500/938] [D loss: 0.1578] [G loss: 4.2841]\n",
            "[Epoch 16/70] [Batch 600/938] [D loss: 0.1364] [G loss: 2.0161]\n",
            "[Epoch 16/70] [Batch 700/938] [D loss: 0.6366] [G loss: 8.0029]\n",
            "[Epoch 16/70] [Batch 800/938] [D loss: 0.1496] [G loss: 2.8271]\n",
            "[Epoch 16/70] [Batch 900/938] [D loss: 0.3083] [G loss: 2.1464]\n",
            "[Epoch 17/70] [Batch 0/938] [D loss: 0.1591] [G loss: 3.1805]\n",
            "[Epoch 17/70] [Batch 100/938] [D loss: 0.7207] [G loss: 7.0604]\n",
            "[Epoch 17/70] [Batch 200/938] [D loss: 0.6132] [G loss: 8.5011]\n",
            "[Epoch 17/70] [Batch 300/938] [D loss: 0.0508] [G loss: 3.4846]\n",
            "[Epoch 17/70] [Batch 400/938] [D loss: 0.1408] [G loss: 2.4062]\n",
            "[Epoch 17/70] [Batch 500/938] [D loss: 0.0989] [G loss: 2.3905]\n",
            "[Epoch 17/70] [Batch 600/938] [D loss: 0.2978] [G loss: 1.3165]\n",
            "[Epoch 17/70] [Batch 700/938] [D loss: 0.3822] [G loss: 0.9681]\n",
            "[Epoch 17/70] [Batch 800/938] [D loss: 0.3301] [G loss: 4.4097]\n",
            "[Epoch 17/70] [Batch 900/938] [D loss: 0.0537] [G loss: 4.3637]\n",
            "[Epoch 18/70] [Batch 0/938] [D loss: 0.2296] [G loss: 2.7918]\n",
            "[Epoch 18/70] [Batch 100/938] [D loss: 0.1875] [G loss: 2.3528]\n",
            "[Epoch 18/70] [Batch 200/938] [D loss: 0.0692] [G loss: 5.0230]\n",
            "[Epoch 18/70] [Batch 300/938] [D loss: 0.1922] [G loss: 1.7916]\n",
            "[Epoch 18/70] [Batch 400/938] [D loss: 0.0975] [G loss: 3.2937]\n",
            "[Epoch 18/70] [Batch 500/938] [D loss: 0.0880] [G loss: 2.1687]\n",
            "[Epoch 18/70] [Batch 600/938] [D loss: 0.1074] [G loss: 2.1215]\n",
            "[Epoch 18/70] [Batch 700/938] [D loss: 0.1463] [G loss: 4.3399]\n",
            "[Epoch 18/70] [Batch 800/938] [D loss: 0.1230] [G loss: 2.6545]\n",
            "[Epoch 18/70] [Batch 900/938] [D loss: 0.1963] [G loss: 2.8810]\n",
            "[Epoch 19/70] [Batch 0/938] [D loss: 0.1440] [G loss: 1.9679]\n",
            "[Epoch 19/70] [Batch 100/938] [D loss: 0.2692] [G loss: 1.4185]\n",
            "[Epoch 19/70] [Batch 200/938] [D loss: 0.0680] [G loss: 3.2646]\n",
            "[Epoch 19/70] [Batch 300/938] [D loss: 0.1058] [G loss: 3.5850]\n",
            "[Epoch 19/70] [Batch 400/938] [D loss: 0.2238] [G loss: 3.8742]\n",
            "[Epoch 19/70] [Batch 500/938] [D loss: 0.2226] [G loss: 3.8018]\n",
            "[Epoch 19/70] [Batch 600/938] [D loss: 0.2017] [G loss: 2.8457]\n",
            "[Epoch 19/70] [Batch 700/938] [D loss: 0.1218] [G loss: 2.1766]\n",
            "[Epoch 19/70] [Batch 800/938] [D loss: 0.1196] [G loss: 2.2304]\n",
            "[Epoch 19/70] [Batch 900/938] [D loss: 0.0590] [G loss: 4.0343]\n",
            "[Epoch 20/70] [Batch 0/938] [D loss: 0.1509] [G loss: 4.2084]\n",
            "[Epoch 20/70] [Batch 100/938] [D loss: 0.2139] [G loss: 1.5532]\n",
            "[Epoch 20/70] [Batch 200/938] [D loss: 1.0154] [G loss: 7.0962]\n",
            "[Epoch 20/70] [Batch 300/938] [D loss: 0.1254] [G loss: 3.3129]\n",
            "[Epoch 20/70] [Batch 400/938] [D loss: 0.1812] [G loss: 1.7221]\n",
            "[Epoch 20/70] [Batch 500/938] [D loss: 0.1163] [G loss: 3.3540]\n",
            "[Epoch 20/70] [Batch 600/938] [D loss: 0.0916] [G loss: 4.3532]\n",
            "[Epoch 20/70] [Batch 700/938] [D loss: 0.1667] [G loss: 2.1963]\n",
            "[Epoch 20/70] [Batch 800/938] [D loss: 0.1842] [G loss: 2.4715]\n",
            "[Epoch 20/70] [Batch 900/938] [D loss: 0.4104] [G loss: 5.5918]\n",
            "[Epoch 21/70] [Batch 0/938] [D loss: 0.3717] [G loss: 0.9381]\n",
            "[Epoch 21/70] [Batch 100/938] [D loss: 0.4104] [G loss: 0.9116]\n",
            "[Epoch 21/70] [Batch 200/938] [D loss: 0.1293] [G loss: 3.2633]\n",
            "[Epoch 21/70] [Batch 300/938] [D loss: 0.1994] [G loss: 3.1226]\n",
            "[Epoch 21/70] [Batch 400/938] [D loss: 0.1713] [G loss: 2.2159]\n",
            "[Epoch 21/70] [Batch 500/938] [D loss: 0.4565] [G loss: 6.1919]\n",
            "[Epoch 21/70] [Batch 600/938] [D loss: 0.1161] [G loss: 2.7472]\n",
            "[Epoch 21/70] [Batch 700/938] [D loss: 0.0951] [G loss: 2.5049]\n",
            "[Epoch 21/70] [Batch 800/938] [D loss: 0.6779] [G loss: 0.4374]\n",
            "[Epoch 21/70] [Batch 900/938] [D loss: 0.1075] [G loss: 3.9087]\n",
            "[Epoch 22/70] [Batch 0/938] [D loss: 0.1040] [G loss: 2.5516]\n",
            "[Epoch 22/70] [Batch 100/938] [D loss: 0.0972] [G loss: 3.0635]\n",
            "[Epoch 22/70] [Batch 200/938] [D loss: 0.2974] [G loss: 1.0113]\n",
            "[Epoch 22/70] [Batch 300/938] [D loss: 0.1833] [G loss: 2.2206]\n",
            "[Epoch 22/70] [Batch 400/938] [D loss: 0.1294] [G loss: 3.2286]\n",
            "[Epoch 22/70] [Batch 500/938] [D loss: 0.2005] [G loss: 2.3708]\n",
            "[Epoch 22/70] [Batch 600/938] [D loss: 0.1161] [G loss: 2.1316]\n",
            "[Epoch 22/70] [Batch 700/938] [D loss: 0.1504] [G loss: 4.0471]\n",
            "[Epoch 22/70] [Batch 800/938] [D loss: 0.0726] [G loss: 3.6071]\n",
            "[Epoch 22/70] [Batch 900/938] [D loss: 0.0995] [G loss: 2.3362]\n",
            "[Epoch 23/70] [Batch 0/938] [D loss: 0.0958] [G loss: 3.1668]\n",
            "[Epoch 23/70] [Batch 100/938] [D loss: 0.4562] [G loss: 7.2169]\n",
            "[Epoch 23/70] [Batch 200/938] [D loss: 0.1599] [G loss: 1.6678]\n",
            "[Epoch 23/70] [Batch 300/938] [D loss: 0.1612] [G loss: 2.6802]\n",
            "[Epoch 23/70] [Batch 400/938] [D loss: 0.1740] [G loss: 5.9601]\n",
            "[Epoch 23/70] [Batch 500/938] [D loss: 0.1866] [G loss: 4.4357]\n",
            "[Epoch 23/70] [Batch 600/938] [D loss: 0.3357] [G loss: 6.5624]\n",
            "[Epoch 23/70] [Batch 700/938] [D loss: 0.2932] [G loss: 2.4807]\n",
            "[Epoch 23/70] [Batch 800/938] [D loss: 0.1632] [G loss: 3.7835]\n",
            "[Epoch 23/70] [Batch 900/938] [D loss: 0.1610] [G loss: 3.7063]\n",
            "[Epoch 24/70] [Batch 0/938] [D loss: 0.0918] [G loss: 3.0821]\n",
            "[Epoch 24/70] [Batch 100/938] [D loss: 0.1825] [G loss: 4.7058]\n",
            "[Epoch 24/70] [Batch 200/938] [D loss: 0.1170] [G loss: 3.0515]\n",
            "[Epoch 24/70] [Batch 300/938] [D loss: 0.1194] [G loss: 2.4202]\n",
            "[Epoch 24/70] [Batch 400/938] [D loss: 0.1009] [G loss: 2.3259]\n",
            "[Epoch 24/70] [Batch 500/938] [D loss: 0.1363] [G loss: 2.3788]\n",
            "[Epoch 24/70] [Batch 600/938] [D loss: 0.1878] [G loss: 1.7052]\n",
            "[Epoch 24/70] [Batch 700/938] [D loss: 0.1031] [G loss: 2.6107]\n",
            "[Epoch 24/70] [Batch 800/938] [D loss: 0.2017] [G loss: 3.7674]\n",
            "[Epoch 24/70] [Batch 900/938] [D loss: 0.2004] [G loss: 4.0203]\n",
            "[Epoch 25/70] [Batch 0/938] [D loss: 0.1413] [G loss: 1.7777]\n",
            "[Epoch 25/70] [Batch 100/938] [D loss: 0.0967] [G loss: 3.1888]\n",
            "[Epoch 25/70] [Batch 200/938] [D loss: 0.1942] [G loss: 1.5375]\n",
            "[Epoch 25/70] [Batch 300/938] [D loss: 0.1161] [G loss: 2.1694]\n",
            "[Epoch 25/70] [Batch 400/938] [D loss: 0.1617] [G loss: 2.8054]\n",
            "[Epoch 25/70] [Batch 500/938] [D loss: 0.2050] [G loss: 3.3467]\n",
            "[Epoch 25/70] [Batch 600/938] [D loss: 0.1393] [G loss: 3.2625]\n",
            "[Epoch 25/70] [Batch 700/938] [D loss: 1.2677] [G loss: 0.3066]\n",
            "[Epoch 25/70] [Batch 800/938] [D loss: 0.2362] [G loss: 5.3094]\n",
            "[Epoch 25/70] [Batch 900/938] [D loss: 0.1336] [G loss: 3.5131]\n",
            "[Epoch 26/70] [Batch 0/938] [D loss: 0.1033] [G loss: 2.1153]\n",
            "[Epoch 26/70] [Batch 100/938] [D loss: 0.1487] [G loss: 2.2972]\n",
            "[Epoch 26/70] [Batch 200/938] [D loss: 0.1374] [G loss: 2.1851]\n",
            "[Epoch 26/70] [Batch 300/938] [D loss: 0.1132] [G loss: 2.5788]\n",
            "[Epoch 26/70] [Batch 400/938] [D loss: 0.4153] [G loss: 3.6485]\n",
            "[Epoch 26/70] [Batch 500/938] [D loss: 0.1058] [G loss: 2.6960]\n",
            "[Epoch 26/70] [Batch 600/938] [D loss: 0.1502] [G loss: 2.2075]\n",
            "[Epoch 26/70] [Batch 700/938] [D loss: 0.1157] [G loss: 2.1858]\n",
            "[Epoch 26/70] [Batch 800/938] [D loss: 0.1653] [G loss: 3.2303]\n",
            "[Epoch 26/70] [Batch 900/938] [D loss: 0.1264] [G loss: 2.5943]\n",
            "[Epoch 27/70] [Batch 0/938] [D loss: 0.3670] [G loss: 1.2912]\n",
            "[Epoch 27/70] [Batch 100/938] [D loss: 0.1577] [G loss: 3.5181]\n",
            "[Epoch 27/70] [Batch 200/938] [D loss: 0.1075] [G loss: 2.5862]\n",
            "[Epoch 27/70] [Batch 300/938] [D loss: 0.1991] [G loss: 4.5491]\n",
            "[Epoch 27/70] [Batch 400/938] [D loss: 0.1134] [G loss: 3.2516]\n",
            "[Epoch 27/70] [Batch 500/938] [D loss: 0.2482] [G loss: 3.2964]\n",
            "[Epoch 27/70] [Batch 600/938] [D loss: 0.1448] [G loss: 2.0563]\n",
            "[Epoch 27/70] [Batch 700/938] [D loss: 0.1825] [G loss: 3.3857]\n",
            "[Epoch 27/70] [Batch 800/938] [D loss: 0.2184] [G loss: 2.8551]\n",
            "[Epoch 27/70] [Batch 900/938] [D loss: 0.1255] [G loss: 5.0081]\n",
            "[Epoch 28/70] [Batch 0/938] [D loss: 0.1490] [G loss: 4.6513]\n",
            "[Epoch 28/70] [Batch 100/938] [D loss: 0.1149] [G loss: 3.0371]\n",
            "[Epoch 28/70] [Batch 200/938] [D loss: 0.2725] [G loss: 6.8878]\n",
            "[Epoch 28/70] [Batch 300/938] [D loss: 0.4788] [G loss: 7.0398]\n",
            "[Epoch 28/70] [Batch 400/938] [D loss: 0.2398] [G loss: 1.8876]\n",
            "[Epoch 28/70] [Batch 500/938] [D loss: 0.2094] [G loss: 1.4286]\n",
            "[Epoch 28/70] [Batch 600/938] [D loss: 0.0838] [G loss: 3.2648]\n",
            "[Epoch 28/70] [Batch 700/938] [D loss: 0.1782] [G loss: 4.2716]\n",
            "[Epoch 28/70] [Batch 800/938] [D loss: 0.0941] [G loss: 4.2537]\n",
            "[Epoch 28/70] [Batch 900/938] [D loss: 0.0992] [G loss: 3.1260]\n",
            "[Epoch 29/70] [Batch 0/938] [D loss: 0.1942] [G loss: 2.4304]\n",
            "[Epoch 29/70] [Batch 100/938] [D loss: 0.1172] [G loss: 4.6192]\n",
            "[Epoch 29/70] [Batch 200/938] [D loss: 0.1720] [G loss: 1.5383]\n",
            "[Epoch 29/70] [Batch 300/938] [D loss: 0.1615] [G loss: 2.3214]\n",
            "[Epoch 29/70] [Batch 400/938] [D loss: 0.1513] [G loss: 3.8092]\n",
            "[Epoch 29/70] [Batch 500/938] [D loss: 0.2036] [G loss: 3.5698]\n",
            "[Epoch 29/70] [Batch 600/938] [D loss: 0.3225] [G loss: 1.6155]\n",
            "[Epoch 29/70] [Batch 700/938] [D loss: 0.0961] [G loss: 2.6024]\n",
            "[Epoch 29/70] [Batch 800/938] [D loss: 0.2863] [G loss: 2.2043]\n",
            "[Epoch 29/70] [Batch 900/938] [D loss: 0.1118] [G loss: 2.7723]\n",
            "[Epoch 30/70] [Batch 0/938] [D loss: 0.1490] [G loss: 2.2851]\n",
            "[Epoch 30/70] [Batch 100/938] [D loss: 0.1269] [G loss: 2.4037]\n",
            "[Epoch 30/70] [Batch 200/938] [D loss: 0.1414] [G loss: 1.6375]\n",
            "[Epoch 30/70] [Batch 300/938] [D loss: 0.2283] [G loss: 3.0533]\n",
            "[Epoch 30/70] [Batch 400/938] [D loss: 0.1357] [G loss: 2.9041]\n",
            "[Epoch 30/70] [Batch 500/938] [D loss: 0.2412] [G loss: 1.6125]\n",
            "[Epoch 30/70] [Batch 600/938] [D loss: 0.0972] [G loss: 3.5833]\n",
            "[Epoch 30/70] [Batch 700/938] [D loss: 0.1797] [G loss: 6.0657]\n",
            "[Epoch 30/70] [Batch 800/938] [D loss: 0.1772] [G loss: 5.0382]\n",
            "[Epoch 30/70] [Batch 900/938] [D loss: 0.1042] [G loss: 4.7162]\n",
            "[Epoch 31/70] [Batch 0/938] [D loss: 0.1404] [G loss: 2.4987]\n",
            "[Epoch 31/70] [Batch 100/938] [D loss: 0.1288] [G loss: 4.4638]\n",
            "[Epoch 31/70] [Batch 200/938] [D loss: 0.1398] [G loss: 1.5776]\n",
            "[Epoch 31/70] [Batch 300/938] [D loss: 0.3866] [G loss: 8.7592]\n",
            "[Epoch 31/70] [Batch 400/938] [D loss: 0.0851] [G loss: 2.9299]\n",
            "[Epoch 31/70] [Batch 500/938] [D loss: 0.2044] [G loss: 1.5994]\n",
            "[Epoch 31/70] [Batch 600/938] [D loss: 0.1338] [G loss: 3.3286]\n",
            "[Epoch 31/70] [Batch 700/938] [D loss: 0.2391] [G loss: 4.0537]\n",
            "[Epoch 31/70] [Batch 800/938] [D loss: 0.0898] [G loss: 2.3403]\n",
            "[Epoch 31/70] [Batch 900/938] [D loss: 0.0766] [G loss: 3.1574]\n",
            "[Epoch 32/70] [Batch 0/938] [D loss: 0.5525] [G loss: 7.0472]\n",
            "[Epoch 32/70] [Batch 100/938] [D loss: 0.2166] [G loss: 4.0267]\n",
            "[Epoch 32/70] [Batch 200/938] [D loss: 0.1320] [G loss: 4.2581]\n",
            "[Epoch 32/70] [Batch 300/938] [D loss: 0.2909] [G loss: 1.6136]\n",
            "[Epoch 32/70] [Batch 400/938] [D loss: 0.1420] [G loss: 2.1488]\n",
            "[Epoch 32/70] [Batch 500/938] [D loss: 0.1865] [G loss: 3.1133]\n",
            "[Epoch 32/70] [Batch 600/938] [D loss: 0.5471] [G loss: 11.5198]\n",
            "[Epoch 32/70] [Batch 700/938] [D loss: 0.1338] [G loss: 4.0278]\n",
            "[Epoch 32/70] [Batch 800/938] [D loss: 0.0919] [G loss: 3.1051]\n",
            "[Epoch 32/70] [Batch 900/938] [D loss: 0.1116] [G loss: 2.4917]\n",
            "[Epoch 33/70] [Batch 0/938] [D loss: 0.1345] [G loss: 2.3635]\n",
            "[Epoch 33/70] [Batch 100/938] [D loss: 0.1421] [G loss: 7.1884]\n",
            "[Epoch 33/70] [Batch 200/938] [D loss: 0.1240] [G loss: 4.0528]\n",
            "[Epoch 33/70] [Batch 300/938] [D loss: 0.1779] [G loss: 2.7392]\n",
            "[Epoch 33/70] [Batch 400/938] [D loss: 0.1305] [G loss: 3.9296]\n",
            "[Epoch 33/70] [Batch 500/938] [D loss: 0.1544] [G loss: 2.2193]\n",
            "[Epoch 33/70] [Batch 600/938] [D loss: 0.1167] [G loss: 2.2119]\n",
            "[Epoch 33/70] [Batch 700/938] [D loss: 0.1483] [G loss: 2.3715]\n",
            "[Epoch 33/70] [Batch 800/938] [D loss: 0.4108] [G loss: 1.1479]\n",
            "[Epoch 33/70] [Batch 900/938] [D loss: 0.1513] [G loss: 3.0234]\n",
            "[Epoch 34/70] [Batch 0/938] [D loss: 0.1210] [G loss: 4.8660]\n",
            "[Epoch 34/70] [Batch 100/938] [D loss: 0.1460] [G loss: 3.0083]\n",
            "[Epoch 34/70] [Batch 200/938] [D loss: 0.1942] [G loss: 1.8330]\n",
            "[Epoch 34/70] [Batch 300/938] [D loss: 0.2699] [G loss: 1.1266]\n",
            "[Epoch 34/70] [Batch 400/938] [D loss: 0.1568] [G loss: 2.8438]\n",
            "[Epoch 34/70] [Batch 500/938] [D loss: 0.2109] [G loss: 1.4849]\n",
            "[Epoch 34/70] [Batch 600/938] [D loss: 0.1279] [G loss: 3.8895]\n",
            "[Epoch 34/70] [Batch 700/938] [D loss: 0.1740] [G loss: 3.2028]\n",
            "[Epoch 34/70] [Batch 800/938] [D loss: 0.1898] [G loss: 3.4566]\n",
            "[Epoch 34/70] [Batch 900/938] [D loss: 0.2690] [G loss: 1.4439]\n",
            "[Epoch 35/70] [Batch 0/938] [D loss: 0.1087] [G loss: 2.5179]\n",
            "[Epoch 35/70] [Batch 100/938] [D loss: 0.2039] [G loss: 6.2088]\n",
            "[Epoch 35/70] [Batch 200/938] [D loss: 0.2251] [G loss: 2.6843]\n",
            "[Epoch 35/70] [Batch 300/938] [D loss: 0.1445] [G loss: 2.3275]\n",
            "[Epoch 35/70] [Batch 400/938] [D loss: 0.0734] [G loss: 5.5425]\n",
            "[Epoch 35/70] [Batch 500/938] [D loss: 0.1453] [G loss: 2.2921]\n",
            "[Epoch 35/70] [Batch 600/938] [D loss: 0.1671] [G loss: 1.8445]\n",
            "[Epoch 35/70] [Batch 700/938] [D loss: 0.0892] [G loss: 3.0736]\n",
            "[Epoch 35/70] [Batch 800/938] [D loss: 0.1173] [G loss: 4.2503]\n",
            "[Epoch 35/70] [Batch 900/938] [D loss: 0.1743] [G loss: 3.4183]\n",
            "[Epoch 36/70] [Batch 0/938] [D loss: 0.1221] [G loss: 2.6420]\n",
            "[Epoch 36/70] [Batch 100/938] [D loss: 0.1987] [G loss: 3.6388]\n",
            "[Epoch 36/70] [Batch 200/938] [D loss: 0.1671] [G loss: 2.8117]\n",
            "[Epoch 36/70] [Batch 300/938] [D loss: 0.1052] [G loss: 2.6332]\n",
            "[Epoch 36/70] [Batch 400/938] [D loss: 0.1329] [G loss: 3.2262]\n",
            "[Epoch 36/70] [Batch 500/938] [D loss: 0.1048] [G loss: 4.2902]\n",
            "[Epoch 36/70] [Batch 600/938] [D loss: 0.1298] [G loss: 2.0186]\n",
            "[Epoch 36/70] [Batch 700/938] [D loss: 0.2799] [G loss: 8.0962]\n",
            "[Epoch 36/70] [Batch 800/938] [D loss: 0.3211] [G loss: 4.3762]\n",
            "[Epoch 36/70] [Batch 900/938] [D loss: 0.1676] [G loss: 2.8421]\n",
            "[Epoch 37/70] [Batch 0/938] [D loss: 0.2017] [G loss: 1.9806]\n",
            "[Epoch 37/70] [Batch 100/938] [D loss: 0.1636] [G loss: 3.9085]\n",
            "[Epoch 37/70] [Batch 200/938] [D loss: 0.2272] [G loss: 2.8220]\n",
            "[Epoch 37/70] [Batch 300/938] [D loss: 0.1963] [G loss: 3.1218]\n",
            "[Epoch 37/70] [Batch 400/938] [D loss: 0.1862] [G loss: 3.3272]\n",
            "[Epoch 37/70] [Batch 500/938] [D loss: 0.1588] [G loss: 1.6768]\n",
            "[Epoch 37/70] [Batch 600/938] [D loss: 0.1021] [G loss: 4.5180]\n",
            "[Epoch 37/70] [Batch 700/938] [D loss: 0.2671] [G loss: 2.9878]\n",
            "[Epoch 37/70] [Batch 800/938] [D loss: 0.1573] [G loss: 5.3474]\n",
            "[Epoch 37/70] [Batch 900/938] [D loss: 0.3125] [G loss: 4.1150]\n",
            "[Epoch 38/70] [Batch 0/938] [D loss: 0.2143] [G loss: 4.3875]\n",
            "[Epoch 38/70] [Batch 100/938] [D loss: 0.3127] [G loss: 4.1896]\n",
            "[Epoch 38/70] [Batch 200/938] [D loss: 0.2529] [G loss: 1.5772]\n",
            "[Epoch 38/70] [Batch 300/938] [D loss: 0.2417] [G loss: 3.3341]\n",
            "[Epoch 38/70] [Batch 400/938] [D loss: 0.1710] [G loss: 3.8671]\n",
            "[Epoch 38/70] [Batch 500/938] [D loss: 0.1594] [G loss: 3.5664]\n",
            "[Epoch 38/70] [Batch 600/938] [D loss: 0.2008] [G loss: 2.4798]\n",
            "[Epoch 38/70] [Batch 700/938] [D loss: 0.1690] [G loss: 1.8581]\n",
            "[Epoch 38/70] [Batch 800/938] [D loss: 0.2462] [G loss: 2.1729]\n",
            "[Epoch 38/70] [Batch 900/938] [D loss: 0.1722] [G loss: 2.6373]\n",
            "[Epoch 39/70] [Batch 0/938] [D loss: 0.1395] [G loss: 4.2443]\n",
            "[Epoch 39/70] [Batch 100/938] [D loss: 0.1716] [G loss: 2.5712]\n",
            "[Epoch 39/70] [Batch 200/938] [D loss: 0.2399] [G loss: 4.6373]\n",
            "[Epoch 39/70] [Batch 300/938] [D loss: 0.1131] [G loss: 2.1471]\n",
            "[Epoch 39/70] [Batch 400/938] [D loss: 0.1454] [G loss: 2.2351]\n",
            "[Epoch 39/70] [Batch 500/938] [D loss: 0.0973] [G loss: 2.8082]\n",
            "[Epoch 39/70] [Batch 600/938] [D loss: 0.2450] [G loss: 2.2068]\n",
            "[Epoch 39/70] [Batch 700/938] [D loss: 0.2691] [G loss: 2.1478]\n",
            "[Epoch 39/70] [Batch 800/938] [D loss: 0.2314] [G loss: 2.8339]\n",
            "[Epoch 39/70] [Batch 900/938] [D loss: 0.1513] [G loss: 2.5142]\n",
            "[Epoch 40/70] [Batch 0/938] [D loss: 0.1415] [G loss: 2.6992]\n",
            "[Epoch 40/70] [Batch 100/938] [D loss: 0.1342] [G loss: 2.9111]\n",
            "[Epoch 40/70] [Batch 200/938] [D loss: 0.1912] [G loss: 2.3394]\n",
            "[Epoch 40/70] [Batch 300/938] [D loss: 0.1363] [G loss: 3.1379]\n",
            "[Epoch 40/70] [Batch 400/938] [D loss: 0.1510] [G loss: 2.0942]\n",
            "[Epoch 40/70] [Batch 500/938] [D loss: 0.1096] [G loss: 5.7863]\n",
            "[Epoch 40/70] [Batch 600/938] [D loss: 0.2062] [G loss: 2.4900]\n",
            "[Epoch 40/70] [Batch 700/938] [D loss: 0.0748] [G loss: 3.7080]\n",
            "[Epoch 40/70] [Batch 800/938] [D loss: 0.1283] [G loss: 3.9700]\n",
            "[Epoch 40/70] [Batch 900/938] [D loss: 0.1331] [G loss: 2.8572]\n",
            "[Epoch 41/70] [Batch 0/938] [D loss: 0.1998] [G loss: 6.0300]\n",
            "[Epoch 41/70] [Batch 100/938] [D loss: 0.0464] [G loss: 3.1961]\n",
            "[Epoch 41/70] [Batch 200/938] [D loss: 0.2304] [G loss: 1.5837]\n",
            "[Epoch 41/70] [Batch 300/938] [D loss: 0.1872] [G loss: 3.7196]\n",
            "[Epoch 41/70] [Batch 400/938] [D loss: 0.0732] [G loss: 3.5823]\n",
            "[Epoch 41/70] [Batch 500/938] [D loss: 0.0861] [G loss: 3.0136]\n",
            "[Epoch 41/70] [Batch 600/938] [D loss: 0.1655] [G loss: 2.2370]\n",
            "[Epoch 41/70] [Batch 700/938] [D loss: 0.1477] [G loss: 2.4236]\n",
            "[Epoch 41/70] [Batch 800/938] [D loss: 0.1341] [G loss: 3.0639]\n",
            "[Epoch 41/70] [Batch 900/938] [D loss: 0.1428] [G loss: 6.9720]\n",
            "[Epoch 42/70] [Batch 0/938] [D loss: 0.1946] [G loss: 3.1548]\n",
            "[Epoch 42/70] [Batch 100/938] [D loss: 0.0955] [G loss: 3.1977]\n",
            "[Epoch 42/70] [Batch 200/938] [D loss: 0.0994] [G loss: 2.8621]\n",
            "[Epoch 42/70] [Batch 300/938] [D loss: 0.1354] [G loss: 3.8023]\n",
            "[Epoch 42/70] [Batch 400/938] [D loss: 0.2260] [G loss: 4.7142]\n",
            "[Epoch 42/70] [Batch 500/938] [D loss: 0.1193] [G loss: 2.6967]\n",
            "[Epoch 42/70] [Batch 600/938] [D loss: 0.1595] [G loss: 3.0569]\n",
            "[Epoch 42/70] [Batch 700/938] [D loss: 0.1500] [G loss: 4.7867]\n",
            "[Epoch 42/70] [Batch 800/938] [D loss: 0.2461] [G loss: 5.4082]\n",
            "[Epoch 42/70] [Batch 900/938] [D loss: 0.0460] [G loss: 3.9522]\n",
            "[Epoch 43/70] [Batch 0/938] [D loss: 0.3748] [G loss: 4.6117]\n",
            "[Epoch 43/70] [Batch 100/938] [D loss: 0.2714] [G loss: 6.3463]\n",
            "[Epoch 43/70] [Batch 200/938] [D loss: 0.2077] [G loss: 1.8169]\n",
            "[Epoch 43/70] [Batch 300/938] [D loss: 0.1313] [G loss: 2.9663]\n",
            "[Epoch 43/70] [Batch 400/938] [D loss: 0.0675] [G loss: 2.9084]\n",
            "[Epoch 43/70] [Batch 500/938] [D loss: 0.1188] [G loss: 2.2774]\n",
            "[Epoch 43/70] [Batch 600/938] [D loss: 0.2612] [G loss: 1.2229]\n",
            "[Epoch 43/70] [Batch 700/938] [D loss: 0.1452] [G loss: 3.3624]\n",
            "[Epoch 43/70] [Batch 800/938] [D loss: 0.1649] [G loss: 2.5460]\n",
            "[Epoch 43/70] [Batch 900/938] [D loss: 0.0960] [G loss: 2.9581]\n",
            "[Epoch 44/70] [Batch 0/938] [D loss: 0.2202] [G loss: 2.5537]\n",
            "[Epoch 44/70] [Batch 100/938] [D loss: 0.0839] [G loss: 2.4745]\n",
            "[Epoch 44/70] [Batch 200/938] [D loss: 0.1860] [G loss: 2.0167]\n",
            "[Epoch 44/70] [Batch 300/938] [D loss: 0.1237] [G loss: 3.0017]\n",
            "[Epoch 44/70] [Batch 400/938] [D loss: 0.1830] [G loss: 3.3480]\n",
            "[Epoch 44/70] [Batch 500/938] [D loss: 0.7935] [G loss: 8.3684]\n",
            "[Epoch 44/70] [Batch 600/938] [D loss: 0.1009] [G loss: 4.5669]\n",
            "[Epoch 44/70] [Batch 700/938] [D loss: 0.1948] [G loss: 6.7997]\n",
            "[Epoch 44/70] [Batch 800/938] [D loss: 0.1402] [G loss: 2.6580]\n",
            "[Epoch 44/70] [Batch 900/938] [D loss: 0.1584] [G loss: 2.8033]\n",
            "[Epoch 45/70] [Batch 0/938] [D loss: 0.2114] [G loss: 1.6156]\n",
            "[Epoch 45/70] [Batch 100/938] [D loss: 0.1196] [G loss: 3.7494]\n",
            "[Epoch 45/70] [Batch 200/938] [D loss: 0.0494] [G loss: 3.4959]\n",
            "[Epoch 45/70] [Batch 300/938] [D loss: 0.1297] [G loss: 3.1299]\n",
            "[Epoch 45/70] [Batch 400/938] [D loss: 0.0892] [G loss: 2.9282]\n",
            "[Epoch 45/70] [Batch 500/938] [D loss: 0.1465] [G loss: 5.4864]\n",
            "[Epoch 45/70] [Batch 600/938] [D loss: 0.1026] [G loss: 3.0687]\n",
            "[Epoch 45/70] [Batch 700/938] [D loss: 0.0955] [G loss: 2.8530]\n",
            "[Epoch 45/70] [Batch 800/938] [D loss: 0.2369] [G loss: 2.1029]\n",
            "[Epoch 45/70] [Batch 900/938] [D loss: 0.0621] [G loss: 4.4555]\n",
            "[Epoch 46/70] [Batch 0/938] [D loss: 0.2306] [G loss: 2.0400]\n",
            "[Epoch 46/70] [Batch 100/938] [D loss: 0.1725] [G loss: 3.1958]\n",
            "[Epoch 46/70] [Batch 200/938] [D loss: 0.0895] [G loss: 3.9170]\n",
            "[Epoch 46/70] [Batch 300/938] [D loss: 0.1538] [G loss: 2.6415]\n",
            "[Epoch 46/70] [Batch 400/938] [D loss: 0.1917] [G loss: 4.2246]\n",
            "[Epoch 46/70] [Batch 500/938] [D loss: 0.1046] [G loss: 2.4312]\n",
            "[Epoch 46/70] [Batch 600/938] [D loss: 0.1247] [G loss: 3.4772]\n",
            "[Epoch 46/70] [Batch 700/938] [D loss: 0.0577] [G loss: 3.5630]\n",
            "[Epoch 46/70] [Batch 800/938] [D loss: 0.1478] [G loss: 1.9897]\n",
            "[Epoch 46/70] [Batch 900/938] [D loss: 0.0742] [G loss: 3.1913]\n",
            "[Epoch 47/70] [Batch 0/938] [D loss: 0.3969] [G loss: 0.8687]\n",
            "[Epoch 47/70] [Batch 100/938] [D loss: 0.0958] [G loss: 2.9402]\n",
            "[Epoch 47/70] [Batch 200/938] [D loss: 0.2870] [G loss: 4.4137]\n",
            "[Epoch 47/70] [Batch 300/938] [D loss: 0.3104] [G loss: 1.4706]\n",
            "[Epoch 47/70] [Batch 400/938] [D loss: 0.1438] [G loss: 5.2049]\n",
            "[Epoch 47/70] [Batch 500/938] [D loss: 0.3846] [G loss: 8.5648]\n",
            "[Epoch 47/70] [Batch 600/938] [D loss: 0.4654] [G loss: 6.8206]\n",
            "[Epoch 47/70] [Batch 700/938] [D loss: 0.1140] [G loss: 3.6173]\n",
            "[Epoch 47/70] [Batch 800/938] [D loss: 0.2774] [G loss: 1.1296]\n",
            "[Epoch 47/70] [Batch 900/938] [D loss: 0.1368] [G loss: 2.3409]\n",
            "[Epoch 48/70] [Batch 0/938] [D loss: 0.1648] [G loss: 1.8286]\n",
            "[Epoch 48/70] [Batch 100/938] [D loss: 0.0900] [G loss: 4.1775]\n",
            "[Epoch 48/70] [Batch 200/938] [D loss: 0.0839] [G loss: 2.8215]\n",
            "[Epoch 48/70] [Batch 300/938] [D loss: 0.2862] [G loss: 7.3615]\n",
            "[Epoch 48/70] [Batch 400/938] [D loss: 0.1574] [G loss: 2.2238]\n",
            "[Epoch 48/70] [Batch 500/938] [D loss: 0.2282] [G loss: 1.4150]\n",
            "[Epoch 48/70] [Batch 600/938] [D loss: 0.1162] [G loss: 3.3831]\n",
            "[Epoch 48/70] [Batch 700/938] [D loss: 0.2276] [G loss: 2.6686]\n",
            "[Epoch 48/70] [Batch 800/938] [D loss: 0.1532] [G loss: 3.5800]\n",
            "[Epoch 48/70] [Batch 900/938] [D loss: 0.2877] [G loss: 4.0701]\n",
            "[Epoch 49/70] [Batch 0/938] [D loss: 0.0690] [G loss: 2.8583]\n",
            "[Epoch 49/70] [Batch 100/938] [D loss: 0.2735] [G loss: 4.0400]\n",
            "[Epoch 49/70] [Batch 200/938] [D loss: 0.2213] [G loss: 1.6732]\n",
            "[Epoch 49/70] [Batch 300/938] [D loss: 0.4418] [G loss: 6.0716]\n",
            "[Epoch 49/70] [Batch 400/938] [D loss: 0.2050] [G loss: 3.6340]\n",
            "[Epoch 49/70] [Batch 500/938] [D loss: 0.1454] [G loss: 3.7381]\n",
            "[Epoch 49/70] [Batch 600/938] [D loss: 0.0553] [G loss: 4.6105]\n",
            "[Epoch 49/70] [Batch 700/938] [D loss: 0.0771] [G loss: 2.7895]\n",
            "[Epoch 49/70] [Batch 800/938] [D loss: 0.1647] [G loss: 1.9347]\n",
            "[Epoch 49/70] [Batch 900/938] [D loss: 0.2175] [G loss: 3.7233]\n",
            "[Epoch 50/70] [Batch 0/938] [D loss: 0.1561] [G loss: 3.5462]\n",
            "[Epoch 50/70] [Batch 100/938] [D loss: 0.1101] [G loss: 2.6333]\n",
            "[Epoch 50/70] [Batch 200/938] [D loss: 0.1347] [G loss: 2.8980]\n",
            "[Epoch 50/70] [Batch 300/938] [D loss: 0.2180] [G loss: 4.0066]\n",
            "[Epoch 50/70] [Batch 400/938] [D loss: 0.0882] [G loss: 2.8441]\n",
            "[Epoch 50/70] [Batch 500/938] [D loss: 0.1568] [G loss: 5.5184]\n",
            "[Epoch 50/70] [Batch 600/938] [D loss: 0.1730] [G loss: 3.5055]\n",
            "[Epoch 50/70] [Batch 700/938] [D loss: 0.0640] [G loss: 3.6755]\n",
            "[Epoch 50/70] [Batch 800/938] [D loss: 0.2135] [G loss: 1.8996]\n",
            "[Epoch 50/70] [Batch 900/938] [D loss: 0.0912] [G loss: 3.1848]\n",
            "[Epoch 51/70] [Batch 0/938] [D loss: 0.1600] [G loss: 2.5328]\n",
            "[Epoch 51/70] [Batch 100/938] [D loss: 0.0997] [G loss: 4.1730]\n",
            "[Epoch 51/70] [Batch 200/938] [D loss: 0.2048] [G loss: 1.6844]\n",
            "[Epoch 51/70] [Batch 300/938] [D loss: 0.2186] [G loss: 3.3006]\n",
            "[Epoch 51/70] [Batch 400/938] [D loss: 0.1367] [G loss: 2.1415]\n",
            "[Epoch 51/70] [Batch 500/938] [D loss: 0.0264] [G loss: 4.1182]\n",
            "[Epoch 51/70] [Batch 600/938] [D loss: 0.0272] [G loss: 3.6328]\n",
            "[Epoch 51/70] [Batch 700/938] [D loss: 0.3267] [G loss: 5.5120]\n",
            "[Epoch 51/70] [Batch 800/938] [D loss: 0.1746] [G loss: 5.1296]\n",
            "[Epoch 51/70] [Batch 900/938] [D loss: 0.1114] [G loss: 2.9536]\n",
            "[Epoch 52/70] [Batch 0/938] [D loss: 0.1052] [G loss: 2.3575]\n",
            "[Epoch 52/70] [Batch 100/938] [D loss: 0.2166] [G loss: 3.4738]\n",
            "[Epoch 52/70] [Batch 200/938] [D loss: 0.1033] [G loss: 3.5465]\n",
            "[Epoch 52/70] [Batch 300/938] [D loss: 0.1060] [G loss: 2.2581]\n",
            "[Epoch 52/70] [Batch 400/938] [D loss: 0.1152] [G loss: 3.6867]\n",
            "[Epoch 52/70] [Batch 500/938] [D loss: 0.3266] [G loss: 7.7888]\n",
            "[Epoch 52/70] [Batch 600/938] [D loss: 0.1394] [G loss: 2.5995]\n",
            "[Epoch 52/70] [Batch 700/938] [D loss: 0.1283] [G loss: 2.9694]\n",
            "[Epoch 52/70] [Batch 800/938] [D loss: 0.1029] [G loss: 5.0410]\n",
            "[Epoch 52/70] [Batch 900/938] [D loss: 0.1966] [G loss: 5.0069]\n",
            "[Epoch 53/70] [Batch 0/938] [D loss: 0.1992] [G loss: 5.1082]\n",
            "[Epoch 53/70] [Batch 100/938] [D loss: 0.2110] [G loss: 5.3021]\n",
            "[Epoch 53/70] [Batch 200/938] [D loss: 0.1316] [G loss: 4.0823]\n",
            "[Epoch 53/70] [Batch 300/938] [D loss: 0.2937] [G loss: 6.5709]\n",
            "[Epoch 53/70] [Batch 400/938] [D loss: 0.1566] [G loss: 1.7524]\n",
            "[Epoch 53/70] [Batch 500/938] [D loss: 0.1086] [G loss: 3.4332]\n",
            "[Epoch 53/70] [Batch 600/938] [D loss: 0.1347] [G loss: 3.7947]\n",
            "[Epoch 53/70] [Batch 700/938] [D loss: 0.1465] [G loss: 2.9703]\n",
            "[Epoch 53/70] [Batch 800/938] [D loss: 0.1461] [G loss: 2.4129]\n",
            "[Epoch 53/70] [Batch 900/938] [D loss: 0.1006] [G loss: 3.6421]\n",
            "[Epoch 54/70] [Batch 0/938] [D loss: 0.0902] [G loss: 4.2607]\n",
            "[Epoch 54/70] [Batch 100/938] [D loss: 0.2022] [G loss: 2.1677]\n",
            "[Epoch 54/70] [Batch 200/938] [D loss: 0.0418] [G loss: 3.5694]\n",
            "[Epoch 54/70] [Batch 300/938] [D loss: 0.1464] [G loss: 2.3719]\n",
            "[Epoch 54/70] [Batch 400/938] [D loss: 0.0985] [G loss: 3.5294]\n",
            "[Epoch 54/70] [Batch 500/938] [D loss: 0.1574] [G loss: 2.4611]\n",
            "[Epoch 54/70] [Batch 600/938] [D loss: 0.1552] [G loss: 2.8301]\n",
            "[Epoch 54/70] [Batch 700/938] [D loss: 0.1914] [G loss: 4.7837]\n",
            "[Epoch 54/70] [Batch 800/938] [D loss: 0.2681] [G loss: 4.9708]\n",
            "[Epoch 54/70] [Batch 900/938] [D loss: 0.2251] [G loss: 2.9207]\n",
            "[Epoch 55/70] [Batch 0/938] [D loss: 0.1107] [G loss: 2.7461]\n",
            "[Epoch 55/70] [Batch 100/938] [D loss: 0.0806] [G loss: 3.9696]\n",
            "[Epoch 55/70] [Batch 200/938] [D loss: 0.1686] [G loss: 2.7955]\n",
            "[Epoch 55/70] [Batch 300/938] [D loss: 0.1919] [G loss: 5.3642]\n",
            "[Epoch 55/70] [Batch 400/938] [D loss: 0.1502] [G loss: 2.2010]\n",
            "[Epoch 55/70] [Batch 500/938] [D loss: 0.1560] [G loss: 3.0287]\n",
            "[Epoch 55/70] [Batch 600/938] [D loss: 0.2497] [G loss: 2.9509]\n",
            "[Epoch 55/70] [Batch 700/938] [D loss: 0.1686] [G loss: 1.5717]\n",
            "[Epoch 55/70] [Batch 800/938] [D loss: 0.2368] [G loss: 3.9146]\n",
            "[Epoch 55/70] [Batch 900/938] [D loss: 0.1590] [G loss: 1.9963]\n",
            "[Epoch 56/70] [Batch 0/938] [D loss: 0.1927] [G loss: 2.5252]\n",
            "[Epoch 56/70] [Batch 100/938] [D loss: 0.1460] [G loss: 1.9852]\n",
            "[Epoch 56/70] [Batch 200/938] [D loss: 0.0891] [G loss: 3.4136]\n",
            "[Epoch 56/70] [Batch 300/938] [D loss: 0.1160] [G loss: 3.0962]\n",
            "[Epoch 56/70] [Batch 400/938] [D loss: 0.1235] [G loss: 2.5497]\n",
            "[Epoch 56/70] [Batch 500/938] [D loss: 0.1531] [G loss: 2.4561]\n",
            "[Epoch 56/70] [Batch 600/938] [D loss: 0.0612] [G loss: 3.2801]\n",
            "[Epoch 56/70] [Batch 700/938] [D loss: 0.1466] [G loss: 3.4069]\n",
            "[Epoch 56/70] [Batch 800/938] [D loss: 0.1147] [G loss: 2.6319]\n",
            "[Epoch 56/70] [Batch 900/938] [D loss: 0.1222] [G loss: 2.8092]\n",
            "[Epoch 57/70] [Batch 0/938] [D loss: 0.1409] [G loss: 3.3593]\n",
            "[Epoch 57/70] [Batch 100/938] [D loss: 0.0796] [G loss: 3.2980]\n",
            "[Epoch 57/70] [Batch 200/938] [D loss: 0.1120] [G loss: 2.8177]\n",
            "[Epoch 57/70] [Batch 300/938] [D loss: 0.0987] [G loss: 2.7950]\n",
            "[Epoch 57/70] [Batch 400/938] [D loss: 0.0971] [G loss: 4.0095]\n",
            "[Epoch 57/70] [Batch 500/938] [D loss: 0.1250] [G loss: 2.9003]\n",
            "[Epoch 57/70] [Batch 600/938] [D loss: 0.0638] [G loss: 3.3323]\n",
            "[Epoch 57/70] [Batch 700/938] [D loss: 0.1781] [G loss: 4.1515]\n",
            "[Epoch 57/70] [Batch 800/938] [D loss: 0.1315] [G loss: 2.8246]\n",
            "[Epoch 57/70] [Batch 900/938] [D loss: 0.1564] [G loss: 2.5817]\n",
            "[Epoch 58/70] [Batch 0/938] [D loss: 0.1526] [G loss: 4.6276]\n",
            "[Epoch 58/70] [Batch 100/938] [D loss: 0.0963] [G loss: 2.3835]\n",
            "[Epoch 58/70] [Batch 200/938] [D loss: 0.1539] [G loss: 2.4005]\n",
            "[Epoch 58/70] [Batch 300/938] [D loss: 0.1902] [G loss: 3.5215]\n",
            "[Epoch 58/70] [Batch 400/938] [D loss: 0.1349] [G loss: 3.6536]\n",
            "[Epoch 58/70] [Batch 500/938] [D loss: 0.2211] [G loss: 4.8798]\n",
            "[Epoch 58/70] [Batch 600/938] [D loss: 0.2232] [G loss: 3.1591]\n",
            "[Epoch 58/70] [Batch 700/938] [D loss: 0.1027] [G loss: 2.7239]\n",
            "[Epoch 58/70] [Batch 800/938] [D loss: 0.1399] [G loss: 3.4527]\n",
            "[Epoch 58/70] [Batch 900/938] [D loss: 0.2095] [G loss: 2.3430]\n",
            "[Epoch 59/70] [Batch 0/938] [D loss: 0.1110] [G loss: 2.1194]\n",
            "[Epoch 59/70] [Batch 100/938] [D loss: 0.1060] [G loss: 4.3325]\n",
            "[Epoch 59/70] [Batch 200/938] [D loss: 0.0834] [G loss: 4.1101]\n",
            "[Epoch 59/70] [Batch 300/938] [D loss: 0.1974] [G loss: 5.3930]\n",
            "[Epoch 59/70] [Batch 400/938] [D loss: 0.2094] [G loss: 2.2285]\n",
            "[Epoch 59/70] [Batch 500/938] [D loss: 0.0982] [G loss: 2.8396]\n",
            "[Epoch 59/70] [Batch 600/938] [D loss: 0.1071] [G loss: 2.9442]\n",
            "[Epoch 59/70] [Batch 700/938] [D loss: 0.1111] [G loss: 3.4392]\n",
            "[Epoch 59/70] [Batch 800/938] [D loss: 0.1619] [G loss: 1.5351]\n",
            "[Epoch 59/70] [Batch 900/938] [D loss: 0.1347] [G loss: 7.4190]\n",
            "[Epoch 60/70] [Batch 0/938] [D loss: 0.1329] [G loss: 2.8139]\n",
            "[Epoch 60/70] [Batch 100/938] [D loss: 0.1883] [G loss: 2.8446]\n",
            "[Epoch 60/70] [Batch 200/938] [D loss: 0.1371] [G loss: 3.5276]\n",
            "[Epoch 60/70] [Batch 300/938] [D loss: 0.1347] [G loss: 2.3383]\n",
            "[Epoch 60/70] [Batch 400/938] [D loss: 0.0593] [G loss: 3.6544]\n",
            "[Epoch 60/70] [Batch 500/938] [D loss: 0.1290] [G loss: 4.3922]\n",
            "[Epoch 60/70] [Batch 600/938] [D loss: 0.2459] [G loss: 7.8158]\n",
            "[Epoch 60/70] [Batch 700/938] [D loss: 0.0738] [G loss: 3.5745]\n",
            "[Epoch 60/70] [Batch 800/938] [D loss: 0.1698] [G loss: 2.3498]\n",
            "[Epoch 60/70] [Batch 900/938] [D loss: 0.3217] [G loss: 2.3294]\n",
            "[Epoch 61/70] [Batch 0/938] [D loss: 0.1646] [G loss: 4.6682]\n",
            "[Epoch 61/70] [Batch 100/938] [D loss: 0.1513] [G loss: 4.1083]\n",
            "[Epoch 61/70] [Batch 200/938] [D loss: 0.0904] [G loss: 3.6826]\n",
            "[Epoch 61/70] [Batch 300/938] [D loss: 0.3684] [G loss: 3.4825]\n",
            "[Epoch 61/70] [Batch 400/938] [D loss: 0.1914] [G loss: 4.2442]\n",
            "[Epoch 61/70] [Batch 500/938] [D loss: 0.1763] [G loss: 2.5561]\n",
            "[Epoch 61/70] [Batch 600/938] [D loss: 0.4106] [G loss: 5.7964]\n",
            "[Epoch 61/70] [Batch 700/938] [D loss: 0.0793] [G loss: 2.5237]\n",
            "[Epoch 61/70] [Batch 800/938] [D loss: 0.0544] [G loss: 3.6406]\n",
            "[Epoch 61/70] [Batch 900/938] [D loss: 0.2438] [G loss: 1.1503]\n",
            "[Epoch 62/70] [Batch 0/938] [D loss: 0.2830] [G loss: 1.4532]\n",
            "[Epoch 62/70] [Batch 100/938] [D loss: 0.1821] [G loss: 4.4411]\n",
            "[Epoch 62/70] [Batch 200/938] [D loss: 0.9186] [G loss: 8.4241]\n",
            "[Epoch 62/70] [Batch 300/938] [D loss: 0.2237] [G loss: 3.0111]\n",
            "[Epoch 62/70] [Batch 400/938] [D loss: 0.1230] [G loss: 2.2521]\n",
            "[Epoch 62/70] [Batch 500/938] [D loss: 0.0952] [G loss: 3.2000]\n",
            "[Epoch 62/70] [Batch 600/938] [D loss: 0.0687] [G loss: 3.6721]\n",
            "[Epoch 62/70] [Batch 700/938] [D loss: 0.2252] [G loss: 1.9032]\n",
            "[Epoch 62/70] [Batch 800/938] [D loss: 0.2136] [G loss: 2.5629]\n",
            "[Epoch 62/70] [Batch 900/938] [D loss: 0.0476] [G loss: 5.1437]\n",
            "[Epoch 63/70] [Batch 0/938] [D loss: 0.2272] [G loss: 1.4077]\n",
            "[Epoch 63/70] [Batch 100/938] [D loss: 0.1236] [G loss: 2.3527]\n",
            "[Epoch 63/70] [Batch 200/938] [D loss: 0.2242] [G loss: 5.7980]\n",
            "[Epoch 63/70] [Batch 300/938] [D loss: 0.0787] [G loss: 2.9670]\n",
            "[Epoch 63/70] [Batch 400/938] [D loss: 0.1089] [G loss: 3.0538]\n",
            "[Epoch 63/70] [Batch 500/938] [D loss: 0.0792] [G loss: 3.2193]\n",
            "[Epoch 63/70] [Batch 600/938] [D loss: 0.1080] [G loss: 3.3017]\n",
            "[Epoch 63/70] [Batch 700/938] [D loss: 0.1627] [G loss: 2.9936]\n",
            "[Epoch 63/70] [Batch 800/938] [D loss: 0.0586] [G loss: 3.4548]\n",
            "[Epoch 63/70] [Batch 900/938] [D loss: 0.1504] [G loss: 2.7405]\n",
            "[Epoch 64/70] [Batch 0/938] [D loss: 0.0723] [G loss: 3.2422]\n",
            "[Epoch 64/70] [Batch 100/938] [D loss: 0.1307] [G loss: 2.9861]\n",
            "[Epoch 64/70] [Batch 200/938] [D loss: 0.2425] [G loss: 7.1527]\n",
            "[Epoch 64/70] [Batch 300/938] [D loss: 0.1112] [G loss: 2.0498]\n",
            "[Epoch 64/70] [Batch 400/938] [D loss: 0.1891] [G loss: 1.7517]\n",
            "[Epoch 64/70] [Batch 500/938] [D loss: 0.1451] [G loss: 2.6160]\n",
            "[Epoch 64/70] [Batch 600/938] [D loss: 0.1494] [G loss: 1.9796]\n",
            "[Epoch 64/70] [Batch 700/938] [D loss: 0.1484] [G loss: 2.3281]\n",
            "[Epoch 64/70] [Batch 800/938] [D loss: 0.1843] [G loss: 1.5579]\n",
            "[Epoch 64/70] [Batch 900/938] [D loss: 0.0965] [G loss: 4.7380]\n",
            "[Epoch 65/70] [Batch 0/938] [D loss: 0.0912] [G loss: 3.0779]\n",
            "[Epoch 65/70] [Batch 100/938] [D loss: 0.0226] [G loss: 4.3041]\n",
            "[Epoch 65/70] [Batch 200/938] [D loss: 0.2153] [G loss: 6.3948]\n",
            "[Epoch 65/70] [Batch 300/938] [D loss: 0.1443] [G loss: 3.3177]\n",
            "[Epoch 65/70] [Batch 400/938] [D loss: 0.1763] [G loss: 2.1900]\n",
            "[Epoch 65/70] [Batch 500/938] [D loss: 0.2506] [G loss: 4.6933]\n",
            "[Epoch 65/70] [Batch 600/938] [D loss: 0.1628] [G loss: 5.0635]\n",
            "[Epoch 65/70] [Batch 700/938] [D loss: 0.1049] [G loss: 2.5181]\n",
            "[Epoch 65/70] [Batch 800/938] [D loss: 0.2275] [G loss: 3.2501]\n",
            "[Epoch 65/70] [Batch 900/938] [D loss: 0.2457] [G loss: 5.6520]\n",
            "[Epoch 66/70] [Batch 0/938] [D loss: 0.1739] [G loss: 4.3541]\n",
            "[Epoch 66/70] [Batch 100/938] [D loss: 0.1064] [G loss: 3.6848]\n",
            "[Epoch 66/70] [Batch 200/938] [D loss: 0.1570] [G loss: 2.4071]\n",
            "[Epoch 66/70] [Batch 300/938] [D loss: 0.1069] [G loss: 3.7937]\n",
            "[Epoch 66/70] [Batch 400/938] [D loss: 0.2401] [G loss: 4.7826]\n",
            "[Epoch 66/70] [Batch 500/938] [D loss: 0.1367] [G loss: 4.3253]\n",
            "[Epoch 66/70] [Batch 600/938] [D loss: 0.0799] [G loss: 2.9251]\n",
            "[Epoch 66/70] [Batch 700/938] [D loss: 0.1533] [G loss: 3.3395]\n",
            "[Epoch 66/70] [Batch 800/938] [D loss: 0.2405] [G loss: 1.8692]\n",
            "[Epoch 66/70] [Batch 900/938] [D loss: 0.1379] [G loss: 2.5103]\n",
            "[Epoch 67/70] [Batch 0/938] [D loss: 0.0863] [G loss: 2.6109]\n",
            "[Epoch 67/70] [Batch 100/938] [D loss: 0.0957] [G loss: 2.8080]\n",
            "[Epoch 67/70] [Batch 200/938] [D loss: 0.1305] [G loss: 2.4055]\n",
            "[Epoch 67/70] [Batch 300/938] [D loss: 0.2283] [G loss: 4.1032]\n",
            "[Epoch 67/70] [Batch 400/938] [D loss: 0.2136] [G loss: 4.9249]\n",
            "[Epoch 67/70] [Batch 500/938] [D loss: 0.0960] [G loss: 3.0286]\n",
            "[Epoch 67/70] [Batch 600/938] [D loss: 0.0888] [G loss: 3.2972]\n",
            "[Epoch 67/70] [Batch 700/938] [D loss: 0.1353] [G loss: 2.2001]\n",
            "[Epoch 67/70] [Batch 800/938] [D loss: 0.1174] [G loss: 2.4133]\n",
            "[Epoch 67/70] [Batch 900/938] [D loss: 0.1185] [G loss: 3.1849]\n",
            "[Epoch 68/70] [Batch 0/938] [D loss: 0.0779] [G loss: 3.1826]\n",
            "[Epoch 68/70] [Batch 100/938] [D loss: 0.3367] [G loss: 4.9428]\n",
            "[Epoch 68/70] [Batch 200/938] [D loss: 0.1390] [G loss: 2.1686]\n",
            "[Epoch 68/70] [Batch 300/938] [D loss: 0.1034] [G loss: 2.3710]\n",
            "[Epoch 68/70] [Batch 400/938] [D loss: 0.2427] [G loss: 6.1038]\n",
            "[Epoch 68/70] [Batch 500/938] [D loss: 0.1620] [G loss: 1.8974]\n",
            "[Epoch 68/70] [Batch 600/938] [D loss: 0.1136] [G loss: 4.3606]\n",
            "[Epoch 68/70] [Batch 700/938] [D loss: 0.1457] [G loss: 2.7585]\n",
            "[Epoch 68/70] [Batch 800/938] [D loss: 0.2494] [G loss: 2.2091]\n",
            "[Epoch 68/70] [Batch 900/938] [D loss: 0.0952] [G loss: 2.9048]\n",
            "[Epoch 69/70] [Batch 0/938] [D loss: 0.1621] [G loss: 2.8585]\n",
            "[Epoch 69/70] [Batch 100/938] [D loss: 0.1727] [G loss: 3.2432]\n",
            "[Epoch 69/70] [Batch 200/938] [D loss: 0.2353] [G loss: 4.9643]\n",
            "[Epoch 69/70] [Batch 300/938] [D loss: 0.1106] [G loss: 2.8762]\n",
            "[Epoch 69/70] [Batch 400/938] [D loss: 0.2266] [G loss: 4.6553]\n",
            "[Epoch 69/70] [Batch 500/938] [D loss: 0.1320] [G loss: 2.3766]\n",
            "[Epoch 69/70] [Batch 600/938] [D loss: 0.0864] [G loss: 3.2526]\n",
            "[Epoch 69/70] [Batch 700/938] [D loss: 0.1449] [G loss: 2.7257]\n",
            "[Epoch 69/70] [Batch 800/938] [D loss: 0.1227] [G loss: 3.2186]\n",
            "[Epoch 69/70] [Batch 900/938] [D loss: 0.1568] [G loss: 2.4301]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = torch.ones(imgs.size(0), 1, device=device, requires_grad=False)\n",
        "        fake = torch.zeros(imgs.size(0), 1, device=device, requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = imgs.to(device)\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        # Backprop and optimize\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        # Backprop and optimize\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Print progress\n",
        "        if i % 100 == 0:\n",
        "            print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] \" \\\n",
        "                  f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
        "\n",
        "    # Save generated images every epoch\n",
        "    save_image(gen_imgs.data[:25], f\"gan_images/{epoch}.png\", nrow=5, normalize=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQmoGblK4bwn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
